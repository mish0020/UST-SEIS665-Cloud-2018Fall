datapipe.yml
=======================
AWSTemplateFormatVersion: '2010-09-09'
Transform: 'AWS::Serverless-2016-10-31'
Resources:
  DatalogFunction:
    Type: 'AWS::Serverless::Function'
    Properties:
      Handler: datalog.lambda_handler
      Runtime: python3.6
      CodeUri: lambda/datalog.py
      MemorySize: 256
      Timeout: 15
      Role: arn:aws:iam::955851341468:role/mylambda
      Events:
        StreamData:
          Type: Kinesis
          Properties:
            Stream:
              Fn::GetAtt:
                - DataStream
                - Arn
            StartingPosition: TRIM_HORIZON
            BatchSize: 50

  DatastoreFunction:
    Type: 'AWS::Serverless::Function'
    Properties:
      Handler: datastore.db_writer
      Runtime: python3.6
      CodeUri: lambda/datastore.py
      MemorySize: 256
      Timeout: 15
      Role: arn:aws:iam::955851341468:role/mylambda
      Events:
        StreamData:
          Type: Kinesis
          Properties:
            Stream:
              Fn::GetAtt:
                - DataStream
                - Arn
            StartingPosition: TRIM_HORIZON
            BatchSize: 25
      Environment:
        Variables:
          TABLE_NAME:
            Fn::GetAtt:
              - DatapipeTable
              - Arn

  DatapipeTable:
    Type: AWS::Serverless::SimpleTable
    Properties:
      PrimaryKey:
        Name: id
        Type: String
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
      SSESpecification:
        SSEEnabled: true

  DataStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: datastream
      ShardCount: 1
      Tags:
        -
          Key: 'Project'
          Value: 'SEIS665'
          
================================================================
datalog.py
====================================================
import base64


def lambda_handler(event, context):
    for record in event['Records']:
        # Kinesis data is base64 encoded so decode here
        payload = base64.b64decode(record["kinesis"]["data"])
        print("Decoded payload: " + str(payload))
=====================================================================
datastore.py with single write :
===============================================
import base64
import json

import boto3
import os


def db_writer(event, context):

    item = None
    dynamo_db = boto3.resource('dynamodb')

    table_arn = os.environ['TABLE_NAME']
    arn,table_name = table_arn.split("/",1)
    table = dynamo_db.Table(table_name)

    decoded_record_data = [base64.b64decode(record['kinesis']['data']) for record in event['Records']]
    deserialized_data = [json.loads(decoded_record) for decoded_record in decoded_record_data]

    #with table.batch_writer() as batch_writer:
    for item in deserialized_data:
       id = item['id']
       x = item['x']
       y = item['y']
       is_hot = item['is_hot']
       table.put_item(Item={
            'id': id,
            'x':x,
            'y':y,
            'is_hot':is_hot,
          })
       print(json.dumps(item))
=================================================================
datastore.py with batch write
==============================================================
import base64
import json

import boto3
import os


def db_writer(event, context):

    item = None
    dynamo_db = boto3.resource('dynamodb')

    table_arn = os.environ['TABLE_NAME']
    arn,table_name = table_arn.split("/",1)
    table = dynamo_db.Table(table_name)

    decoded_record_data = [base64.b64decode(record['kinesis']['data']) for record in event['Records']]
    deserialized_data = [json.loads(decoded_record) for decoded_record in decoded_record_data]

    with table.batch_writer() as batch_writer:
       for item in deserialized_data:
          batch_writer.put_item(Item=item)
==================================================================
data_generator.py
=================================================================
import boto3
import json
import time
import uuid

from random import random

# Modify this section to reflect your AWS configuration.
awsRegion = "us-east-1"         # The AWS region where your Kinesis Analytics application is configured.
inputStream = "datastream"       # The name of the stream being used as input into the Kinesis Analytics hotspots application

# Variables that control properties of the generated data.
xRange = [0, 10]       # The range of values taken by the x-coordinate
yRange = [0, 10]       # The range of values taken by the y-coordinate
hotspotSideLength = 1  # The side length of the hotspot
hotspotWeight = 0.2    # The fraction ofpoints that are draw from the hotspots

enable_stream = True
clock = uuid.uuid1().clock_seq


def generate_point_in_rectangle(x_min, width, y_min, height):
    """Generate points uniformly in the given rectangle."""
    return {
        'x': str(x_min + random() * width),
        'y': str(y_min + random() * height)
    }


class RecordGenerator(object):
    """A class used to generate points used as input to the hotspot detection algorithm.
    With probability hotspotWeight, a point is drawn from a hotspot, otherwise it is drawn
    from the base distribution. The location of the hotspot changes after every 1000 points
    generated."""

    def __init__(self):
        self.x_min = xRange[0]
        self.width = xRange[1] - xRange[0]
        self.y_min = yRange[0]
        self.height = yRange[1] - yRange[0]
        self.points_generated = 0
        self.hotspot_x_min = None
        self.hotspot_y_min = None

    def get_record(self):
        if self.points_generated % 1000 == 0:
            self.update_hotspot()

        if random() < hotspotWeight:
            record = generate_point_in_rectangle(self.hotspot_x_min,
                                                 hotspotSideLength,
                                                 self.hotspot_y_min,
                                                 hotspotSideLength)
            record['is_hot'] = 'Y'
        else:
            record = generate_point_in_rectangle(self.x_min, self.width, self.y_min, self.height)
            record['is_hot'] = 'N'

        record['id'] = str(clock) + '-' + str(self.points_generated)

        self.points_generated += 1
        data = json.dumps(record)
        print(data)
        return {'Data': bytes(data, 'utf-8'), 'PartitionKey': 'partition_key'}

    def get_records(self, n):
        return [self.get_record() for _ in range(n)]

    def update_hotspot(self):
        self.hotspot_x_min = self.x_min + random() * (self.width - hotspotSideLength)
        self.hotspot_y_min = self.y_min + random() * (self.height - hotspotSideLength)


def main():
    kinesis = boto3.client("kinesis", region_name=awsRegion)

    generator = RecordGenerator()
    batch_size = 10

    i = 1

    while i < 101:
        print(f'batch {i}')
        records = generator.get_records(batch_size)
        if enable_stream:
            response = kinesis.put_records(StreamName=inputStream, Records=records)
            print(response)
        time.sleep(0.1)
        i += 1


if __name__ == "__main__":
    main()
=============================================
assignment11 discussion :
=============================================
jason's  yml file 
Policies:
- AWSLambrd..blah blah
this is how he mentioned : Stream !GetAtt DataStream.Arn
===========================================
with table.batch_writer() as batch
 for record in event['Records']
   payload = base64.b64decode(record['kinesis']['data'])
      batch.put_item(Item=json.loads(payload))
!! That's it !!
